{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BpWg3hJENz0"
   },
   "source": [
    "# CPU vs. GPU Benchmark\n",
    "\n",
    "Let's check out some different PyTorch arithmetic on the CPU vs. GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsKwMKeSSR5"
   },
   "source": [
    "First, import PyTorch and check the version.\n",
    "We will also make sure we can access the CUDA GPU.\n",
    "This should always be your first step!\n",
    "\n",
    "We'll also set a manual_seed for the random operations. While this isn't strictly neccesary for this experiment, it's good practice as it aids with reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ocKjJH0EYb9",
    "outputId": "2a38f8b2-2df1-46d3-a8f9-edac4c138d08"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "# Help with reproducability of test\n",
    "torch.manual_seed(2016)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise OSError(\"ERROR: No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JICBen1CSoPh"
   },
   "source": [
    "## Dot Product\n",
    "\n",
    "Dot products are **extremely** common tensor operations. They are used deep neural networks and linear algebra applications.\n",
    "\n",
    "A dot product is essentially just a bunch of multiplications and additions.\n",
    "\n",
    "PyTorch provides the [`torch.tensordot()`](https://pytorch.org/docs/stable/generated/torch.tensordot.html) method.\n",
    "\n",
    "First, let's define two methods to compute the dot product. One will take place on the CPU and the other on the GPU.\n",
    "\n",
    "### CPU Timing\n",
    "\n",
    "The CPU method is trivial!\n",
    "\n",
    "### GPU Timing\n",
    "\n",
    "The GPU method has a bit more two it. We must:\n",
    "\n",
    "1. Send the tensors to the GPU for computation. We call [`torch.to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) on the tensor to send it to a particular [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)\n",
    "2. Wait for the GPU to synchronize. According to [the docs](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution), GPU ops take place asynchronously so you need to use synchronize for precise timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1eioHVdEJVg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Compute the tensor dot product on CPU\n",
    "def cpu_dot_product(a, b):\n",
    "    return torch.tensordot(a, b)\n",
    "\n",
    "\n",
    "# Send the tensor to GPU then compute dot product\n",
    "# synchronize() required for timing accuracy, see:\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution\n",
    "def gpu_dot_product(a, b):\n",
    "    a_gpu = a.to(\"cuda\")\n",
    "    b_gpu = b.to(\"cuda\")\n",
    "    product = torch.tensordot(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyTqOmp9UBCO"
   },
   "source": [
    "### Running the benchmark\n",
    "\n",
    "This section declares the start and stop tensor sizes for our test.\n",
    "You can change `SIZE_LIMIT` and then run again; just know that at some point you will run out of memory!\n",
    "\n",
    "Next, it does tests at several sizes within this range, doubling each time.\n",
    "\n",
    "We use [`timeit.timeit()`](https://docs.python.org/3/library/timeit.html#timeit.timeit) for the tests. It will call the function multiple times and then average those times. Timeit is also more accurate than manually calling Python's time function and doing subtraction.\n",
    "\n",
    "Finally, results are saved into a list that's then exported to a pandas DataFrame for easy viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsN56pwmJDl3",
    "outputId": "6aa1b237-4d66-4701-fa01-622e015383c6"
   },
   "outputs": [],
   "source": [
    "SIZE_LIMIT = 10000  # where to stop at\n",
    "tensor_size = 10  # start at size 10\n",
    "results = []\n",
    "\n",
    "print(\"Running with 2D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\")\n",
    "\n",
    "# Run the test\n",
    "while tensor_size < SIZE_LIMIT:\n",
    "    # Random array\n",
    "    a = torch.rand(tensor_size, tensor_size)\n",
    "    b = torch.rand(tensor_size, tensor_size)\n",
    "\n",
    "    # Time the CPU operation\n",
    "    cpu_time = timeit.timeit(\"cpu_dot_product(a, b)\", globals=globals(), number=50)\n",
    "\n",
    "    # Time the GPU operation\n",
    "    # First, we send the data to the GPU, called the warm up\n",
    "    # It really depends on the application of this time is important or negligible\n",
    "    # We are doing it here becasue timeit() averages the results of multiple runs\n",
    "    gpu_dot_product(a, b)\n",
    "    # Now we time the actual operation\n",
    "    gpu_time = timeit.timeit(\"gpu_dot_product(a, b)\", globals=globals(), number=50)\n",
    "\n",
    "    # Record the results\n",
    "    results.append(\n",
    "        {\n",
    "            \"tensor_size\": tensor_size,\n",
    "            \"cpu_time\": cpu_time,\n",
    "            \"gpu_time\": gpu_time,\n",
    "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Double tensor_size\n",
    "    tensor_size = tensor_size * 2\n",
    "\n",
    "# Done! Cast the results to a DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OXQbzUWUbqo"
   },
   "source": [
    "### Dot Product Results\n",
    "\n",
    "If you left the default sizes, you should see 10 rows of results.\n",
    "You'll notice that with small tensors the CPU is *faster* than the GPU!\n",
    "This is also indidcated by the **gpu_speedup** being less than 1.\n",
    "\n",
    "But as the tensor sizes grow, the GPU overtakes the CPU for speed! ðŸŽï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBrJm0YQe7K"
   },
   "source": [
    "## Next: Summing a tensor\n",
    "\n",
    "Your task is to repeat this benchmark below, but computing the sum of a single **1D tensor**.\n",
    "\n",
    "Use the [`torch.sum()`](https://pytorch.org/docs/stable/generated/torch.sum.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0jcwTVFWhey"
   },
   "outputs": [],
   "source": [
    "# Define your methods here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt8pxSE9WjdS"
   },
   "outputs": [],
   "source": [
    "# Conduct your benchmark here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b0rM3aeWmcD"
   },
   "source": [
    "### Tensor sum results\n",
    "\n",
    "Jot down some thoughts to yourself here about what you saw ðŸ“ˆ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
