{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byarbrough/ai-hardware/blob/main/book/b1-prediction/kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM7vlRbCh9pw"
      },
      "source": [
        "# ICE: K-Means Clustering\n",
        "\n",
        "## Pre-reading\n",
        "\n",
        "- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html) sections `2.3.1` and `2.3.2`\n",
        "\n",
        "![K-Means Clustering](https://imgs.xkcd.com/comics/k_means_clustering.png)\n",
        "\n",
        "*According to my especially unsupervised K-means clustering algorithm, there are currently about 8 billion types of people in the world.*\n",
        "\n",
        "\n",
        "### Goals\n",
        "\n",
        "- Learn to import and load scikit-learn datasets into numpy arrays\n",
        "- Use [KMeans](https://scikit-learn.org/stable/modules/clustering.html#k-means) to conduct unsupervised learning\n",
        "- Explore the impact of initialization and iterations\n",
        "- Explore a mismatch of `K` and the number of classes\n",
        "\n",
        "This lab is modified from the [Scikit Learn example](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).\n",
        "\n",
        "## Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e13luHdwuS1M"
      },
      "outputs": [],
      "source": [
        "# Google colab includes these by default, so you won't need to run\n",
        "%pip install -q scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8nTVH-dh9pz"
      },
      "source": [
        "We will start by loading the [iris](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) dataset.\n",
        "This dataset contains measurements from the Iris-Setosa, Iris-Versicolour, and Iris-Virginica flowers.\n",
        "\n",
        "Each sample in the dataset contains the following measurements, in centimeters:\n",
        "\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width\n",
        "\n",
        "In the context of clustering, we would like to classify the type of iris based on the measurements of its sepal and petals.\n",
        "\n",
        "*An Iris Setosa*\n",
        "\n",
        "![Iris Setosa](https://live.staticflickr.com/65535/51376589362_b92e27ae7a_b.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3efUU8UMh9p0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data, labels = load_iris(return_X_y=True)\n",
        "n_classes = np.unique(labels).size\n",
        "n_samples, n_features = data.shape\n",
        "\n",
        "print(\n",
        "    f\"# classes: {n_classes}; # samples: {n_samples}; # features per sample: {n_features}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTevpbwfuS1O"
      },
      "source": [
        "Notice that the `load_iris` method returns a tuple, the first element of which is the **data**, the second of which is the **labels**.\n",
        "\n",
        "You can print the numpy array's shape and values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Aoe6HcuS1O"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of numpy data array: {data.shape}\")\n",
        "print(\"Raw data...\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ySHdwT8k2ud"
      },
      "source": [
        "Run this a few times to select and view a random sample from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v_ZUiJBkxeH"
      },
      "outputs": [],
      "source": [
        "from random import randrange\n",
        "\n",
        "sample_id = randrange(n_samples)\n",
        "sample = data[sample_id]\n",
        "\n",
        "# Numpy doesn't contain headers, so extract features via index\n",
        "print(\n",
        "    f\"\"\"Sample {sample_id}:\n",
        "      sepal length = {sample[0]}\n",
        "      sepal width = {sample[1]}\n",
        "      petal length = {sample[2]}\n",
        "      petal width = {sample[3]}\n",
        "      label = {labels[sample_id]}\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLVRzcEjh9p6"
      },
      "source": [
        "### Plot the data\n",
        "\n",
        "#### 4D Plot\n",
        "\n",
        "If we use color to represent the 4th dimension on a 3D plot, we can visualize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnKiXOI4h9pu"
      },
      "outputs": [],
      "source": [
        "# The percent sign runs a terminal command in the current virtual environment\n",
        "# In this case, just help display charts more nicely in the notebook.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GltuuV4ouS1Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Create a 3D plot\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# Use sepal length, sepal width, petal length for x, y, z axes\n",
        "x = data[:, 0]  # Sepal length\n",
        "y = data[:, 1]  # Sepal width\n",
        "z = data[:, 2]  # Petal length\n",
        "\n",
        "# Use petal width as the fourth dimension, represented by color or size\n",
        "c = data[:, 3]  # Petal width\n",
        "\n",
        "markers = [\"o\", \"^\", \"s\"]  # 'o' for setosa, '^' for versicolor, 's' for virginica\n",
        "\n",
        "# Plot the points, with petal width affecting the color and size\n",
        "scatter = ax.scatter(\n",
        "    x, y, z, c=c, cmap=\"viridis\", s=50 + c * 100, edgecolor=\"k\", alpha=0.7\n",
        ")\n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel(\"Sepal Length\")\n",
        "ax.set_ylabel(\"Sepal Width\")\n",
        "ax.set_zlabel(\"Petal Length\")\n",
        "fig.colorbar(scatter, ax=ax, label=\"Petal Width\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2mfUpCuS1Q"
      },
      "source": [
        "We can definitely see some possible clusters, but nothing that we could really select by hand.\n",
        "Honestly, this isn't very helpful.\n",
        "\n",
        "#### PCA-reduced data\n",
        "\n",
        "Do we even need all four of those measurements??\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique who's goal is to reduce the number of features in a dataset while retaining as much variance (information) as possible.\n",
        "\n",
        "The Covariance/ Eigenvector/ Transform of PCA is beyond the scope of this course (try ECE 487!).\n",
        "\n",
        "Just know that the final result is a set of new dimensions that are linear combinations of the original dimensions.\n",
        "\n",
        "For this lab we will use [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "to accomplish the feature reduction. Then we will plot the samples in this new 2D space.\n",
        "\n",
        "Notice that we get two very clear clusters! But we have three classes of flowers..\n",
        "We will add color that matches the labels **but** that's only because we happen to know the labels.\n",
        "K-means is an *unsupervised* algorithm, meaning it works on datasets even if the labels aren't known!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PRcj7onuS1R"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# reduce the 64-dimension data to 2-D\n",
        "pca_data = PCA(n_components=2).fit_transform(data)\n",
        "# simple, non-fancy plot\n",
        "# plt.plot(pca_data[:, 0], pca_data[:, 1], \"k.\", markersize=2)\n",
        "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4z4EJH7qC4z"
      },
      "source": [
        "## Iterate with K-Means\n",
        "\n",
        "First, we'll define a method `plot_kmeans` that will add color, centroids, and lines to our PCA plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKMOFnSxh9p6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def plot_kmeans(reduced_data, kmeans, iteration):\n",
        "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
        "    h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
        "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Obtain labels for each point in mesh. Use last trained model.\n",
        "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Clear previous plot\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    plt.imshow(\n",
        "        Z,\n",
        "        interpolation=\"nearest\",\n",
        "        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "        cmap=plt.cm.Paired,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "    )\n",
        "\n",
        "    plt.scatter(pca_data[:, 0], pca_data[:, 1], s=4, c=labels)\n",
        "    # Plot the centroids as a white X\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    plt.scatter(\n",
        "        centroids[:, 0],\n",
        "        centroids[:, 1],\n",
        "        marker=\"x\",\n",
        "        s=169,\n",
        "        linewidths=3,\n",
        "        color=\"w\",\n",
        "        zorder=10,\n",
        "    )\n",
        "    plt.title(\n",
        "        \"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n",
        "        \"Centroids are marked with white cross\\n\"\n",
        "        \"Iteration: {}\".format(iteration)\n",
        "    )\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN7xw2RkuS1R"
      },
      "source": [
        "### Step through kmeans\n",
        "\n",
        "Before we use the builtin fit method, let's step through what the algorithm is doing. We will do this with [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).\n",
        "\n",
        "This first code block randomly picks centroids and then runs k-means once before plotting.\n",
        "\n",
        "The white **X**'s are the randomly selected centroids of each cluster.\n",
        "Because we didn't specify `random_state` as the seed within `MiniBatchKMeans` this will be different every time we run it.\n",
        "Sometimes that results in better convergence than others!\n",
        "\n",
        "The next code cell does a few iterations, each moving the centroids closer to the center of the nearest cluster.\n",
        "\n",
        "*Run these code blocks a few times to see some different outcomes!* Do you see any instances where the centroids converge on a local minimum that's clearly wrong?\n",
        "\n",
        "Again, remember that k-means doesn't know about the true labels because it's an unsupervised algoritihm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g6Zbu3RuS1R"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "# PCA reduction to 2D\n",
        "reduced_data = PCA(n_components=2).fit_transform(data)\n",
        "\n",
        "# initialize\n",
        "kmeans_step = MiniBatchKMeans(\n",
        "    n_clusters=n_classes, init=\"random\", n_init=1, batch_size=10, max_iter=1\n",
        ")\n",
        "\n",
        "kmeans_step.partial_fit(reduced_data)\n",
        "\n",
        "# dispaly clusters\n",
        "plot_kmeans(reduced_data, kmeans_step, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6xUdM5Uqf2-"
      },
      "outputs": [],
      "source": [
        "max_iterations = 16\n",
        "step_size = 1\n",
        "for i in range(0, max_iterations, step_size):\n",
        "    # step_size iterations between plots\n",
        "    for k in range(step_size):\n",
        "        kmeans_step.partial_fit(reduced_data)\n",
        "\n",
        "    plot_kmeans(reduced_data, kmeans_step, i)\n",
        "    sleep(0.05)\n",
        "\n",
        "plot_kmeans(reduced_data, kmeans_step, max_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9D-rcCtz-CV"
      },
      "source": [
        "## Actual Fit\n",
        "\n",
        "Enough playing around! Let's actually fit the clusters. We'll do the following to help address some issues observed above.\n",
        "\n",
        "1. Instead of specifying the number of iterations, use the [fit](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit)\n",
        "    method to automatically stop when the algorithm **converges**.\n",
        "    This is defined as there being no meaningful change of the centroid locations between rounds.\n",
        "2. Define and employ benchmarks.\n",
        "3. Use `k-means++` to initialize the centroids to be distant from each other.\n",
        "4. Run the algorithm multiple times to compare the random initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVlkqPNwh9p1"
      },
      "source": [
        "### Define our evaluation benchmark\n",
        "\n",
        "Our benchmark will:\n",
        "\n",
        "- create a pipeline which will scale the data using a [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html);\n",
        "- train and time the pipeline fitting;\n",
        "- measure clustering performance via metrics from [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
        "\n",
        "We will use the following metrics:\n",
        "\n",
        "- **Homogeneity Score** is maximized if clusters contain only data points which are members of a single class.\n",
        "- **Completeness Score** is maximized if all the data points that are members of a given class are elements of the same cluster.\n",
        "- **V-measure** is the harmonic mean between homogeneity and completeness, and ultimately what we care about most here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MR6nddjbh9p2"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def bench_k_means(kmeans, name, data, labels):\n",
        "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    kmeans : KMeans instance\n",
        "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
        "        already set.\n",
        "    name : str\n",
        "        Name given to the strategy. It will be used to show the results in a\n",
        "        table.\n",
        "    data : ndarray of shape (n_samples, n_features)\n",
        "        The data to cluster.\n",
        "    labels : ndarray of shape (n_samples,)\n",
        "        The labels used to compute the clustering metrics which requires some\n",
        "        supervision.\n",
        "    \"\"\"\n",
        "    t0 = time()\n",
        "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
        "    fit_time = time() - t0\n",
        "    results = [name, fit_time]\n",
        "\n",
        "    # Define the metrics which require only the true labels and estimator labels\n",
        "    clustering_metrics = [\n",
        "        metrics.homogeneity_score,\n",
        "        metrics.completeness_score,\n",
        "        metrics.v_measure_score,\n",
        "    ]\n",
        "\n",
        "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
        "\n",
        "    # Show the results\n",
        "    formatter_result = \"{:9s}\\t{:.3f}s\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
        "    print(formatter_result.format(*results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNB_8-Leh9p4"
      },
      "source": [
        "### Run the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-6X_gVFh9p5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load a fresh copy of the dataset\n",
        "data, labels = load_iris(return_X_y=True)\n",
        "n_classes = 3  # three types of flowers\n",
        "n_samples, n_features = data.shape\n",
        "\n",
        "print(\n",
        "    f\"# classes: {n_classes}; # samples: {n_samples}; # features per sample: {n_features}\"\n",
        ")\n",
        "\n",
        "print(\"Results for Iris Dataset:\")\n",
        "print(f\"# classes: {n_classes}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "# top bar of stats\n",
        "print(\"Results for\", n_classes, \"clusters\")\n",
        "print(58 * \"_\")\n",
        "print(\"init\\t\\ttime\\thomo\\tcompl\\tv-meas\")\n",
        "\n",
        "# Initialize a K-means algorithm and try on original data\n",
        "kmeans = KMeans(init=\"k-means++\", n_clusters=n_classes, n_init=4, random_state=0)\n",
        "bench_k_means(kmeans=kmeans, name=\"Original\", data=data, labels=labels)\n",
        "\n",
        "# Initialize a fresh K-means algorithm and try on PCA data\n",
        "\n",
        "pca_n_components = 2  # 2 is default because we like x-y charts, but...\n",
        "\n",
        "kmeans_pca = KMeans(init=\"k-means++\", n_clusters=n_classes, n_init=4, random_state=0)\n",
        "reduced_data = PCA(n_components=pca_n_components).fit_transform(data)\n",
        "bench_k_means(\n",
        "    kmeans=kmeans_pca, name=f\"PCA-{pca_n_components}\", data=reduced_data, labels=labels\n",
        ")\n",
        "\n",
        "# bottom bar of stats\n",
        "print(58 * \"_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKw9_fl6uS1T"
      },
      "source": [
        "### Predict\n",
        "\n",
        "Now that we have a fit model, let's make a prediction on a sample!\n",
        "\n",
        "We will use the builtin [predict](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict) method and the results of our PCA-based kmeans model.\n",
        "\n",
        "Because this is an unsupervised clustering algorithm, the clusters number from our true labels probably don't match the ones that kmeans produces!\n",
        "\n",
        "The code below only works with two PCA components and `random_state=0` above; if you change that you'll need to re-match te labels yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK57yfi5uS1T"
      },
      "outputs": [],
      "source": [
        "new_labels = np.zeros_like(labels)\n",
        "\n",
        "for i in range(n_samples):\n",
        "    if labels[i] == 0:\n",
        "        new_labels[i] = 2\n",
        "    elif labels[i] == 1:\n",
        "        new_labels[i] = 0\n",
        "    else:\n",
        "        new_labels[i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcXkReuKuS1T"
      },
      "outputs": [],
      "source": [
        "# Get a random sample\n",
        "sample_id = randrange(n_samples)\n",
        "\n",
        "# Reshape because we are only predicting on a single sample, but KMeans expects a 2D array\n",
        "sample = reduced_data[sample_id].reshape(1, -1)\n",
        "prediction = kmeans_pca.predict(sample).item()\n",
        "print(f\"PCA Kmeans prediction for sample {sample_id}: {prediction}\")\n",
        "print(f\"Correct label for sample {sample_id}: {new_labels[sample_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZRUT3TYuS1T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions for the entire set of samples\n",
        "all_predictions = kmeans_pca.predict(reduced_data)\n",
        "\n",
        "# Confusion matrix\n",
        "confusion = confusion_matrix(new_labels, all_predictions)\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion)\n",
        "\n",
        "accuracy = accuracy_score(all_predictions, new_labels)\n",
        "print(f\"Overall accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRBlM85TuS1T"
      },
      "source": [
        "### Predict\n",
        "\n",
        "Now that we have a fit model, let's make a prediction on a sample!\n",
        "\n",
        "We will use the builtin [predict](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict) method and the results of our PCA-based kmeans model.\n",
        "\n",
        "Because this is an unsupervised clustering algorithm, the clusters number from our true labels probably don't match the ones that kmeans produces!\n",
        "\n",
        "The code below only works with two PCA components and `random_state=0` above; if you change that you'll need to re-match te labels yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9iaTuexuS1T"
      },
      "outputs": [],
      "source": [
        "new_labels = np.zeros_like(labels)\n",
        "\n",
        "for i in range(n_samples):\n",
        "    if labels[i] == 0:\n",
        "        new_labels[i] = 2\n",
        "    elif labels[i] == 1:\n",
        "        new_labels[i] = 0\n",
        "    else:\n",
        "        new_labels[i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03Rz7QguS1T"
      },
      "outputs": [],
      "source": [
        "# Get a random sample\n",
        "sample_id = randrange(n_samples)\n",
        "\n",
        "# Reshape because we are only predicting on a single sample, but KMeans expects a 2D array\n",
        "sample = reduced_data[sample_id].reshape(1, -1)\n",
        "prediction = kmeans_pca.predict(sample).item()\n",
        "print(f\"PCA Kmeans prediction for sample {sample_id}: {prediction}\")\n",
        "print(f\"Correct label for sample {sample_id}: {new_labels[sample_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7nVf-_wuS1T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions for the entire set of samples\n",
        "all_predictions = kmeans_pca.predict(reduced_data)\n",
        "\n",
        "# Confusion matrix\n",
        "confusion = confusion_matrix(new_labels, all_predictions)\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion)\n",
        "\n",
        "accuracy = accuracy_score(all_predictions, new_labels)\n",
        "print(f\"Overall accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMu88_GtuS1T"
      },
      "source": [
        "## ICE Deliverables\n",
        "\n",
        "Now that you have k-means working on super simple dataset, let's try one slightly more complicated!\n",
        "\n",
        "1. Use [load_digits](https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset) to get a dataset of handwritten digits.\n",
        "2. Using the same `bench_k_means` function from above, fit a kmeans with no PCA (hint, pay attention to the number of clusters).\n",
        "3. Using the same `bench_k_means` function from above, fit PCA with 2, 8, and 16 components.\n",
        "4. Answer the questions on Gradescope for this ICE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load a fresh copy of the dataset\n",
        "data, labels = load_digits(return_X_y=True)\n",
        "n_classes = 10  # three types of flowers\n",
        "n_samples, n_features = data.shape\n",
        "\n",
        "print(\n",
        "    f\"# classes: {n_classes}; # samples: {n_samples}; # features per sample: {n_features}\"\n",
        ")\n",
        "\n",
        "print(\"Results for Digits Dataset:\")\n",
        "print(f\"# classes: {n_classes}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "# top bar of stats\n",
        "print(\"Results for\", n_classes, \"clusters\")\n",
        "print(58 * \"_\")\n",
        "print(\"init\\t\\ttime\\thomo\\tcompl\\tv-meas\")\n",
        "\n",
        "# Initialize a K-means algorithm and try on original data\n",
        "kmeans = KMeans(init=\"k-means++\", n_clusters=n_classes, n_init=4, random_state=0)\n",
        "bench_k_means(kmeans=kmeans, name=\"Original\", data=data, labels=labels)\n",
        "\n",
        "# Initialize a fresh K-means algorithm and try on PCA data\n",
        "\n",
        "for pca_n_components in [2, 8, 16, 32, 48, 64]:\n",
        "  kmeans_pca = KMeans(init=\"k-means++\", n_clusters=n_classes, n_init=4, random_state=0)\n",
        "  reduced_data = PCA(n_components=pca_n_components).fit_transform(data)\n",
        "  bench_k_means(\n",
        "    kmeans=kmeans_pca, name=f\"PCA-{pca_n_components}\", data=reduced_data, labels=labels\n",
        "    )\n",
        "\n",
        "# bottom bar of stats\n",
        "print(58 * \"_\")"
      ],
      "metadata": {
        "id": "pawwcG1YuVj6",
        "outputId": "a84b84f5-7391-4cd1-defa-eba166da5bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# classes: 10; # samples: 1797; # features per sample: 64\n",
            "Results for Digits Dataset:\n",
            "# classes: 10; # samples: 1797; # features 64\n",
            "Results for 10 clusters\n",
            "__________________________________________________________\n",
            "init\t\ttime\thomo\tcompl\tv-meas\n",
            "Original \t0.183s\t0.598\t0.645\t0.621\n",
            "PCA-2    \t0.081s\t0.523\t0.527\t0.525\n",
            "PCA-8    \t0.109s\t0.682\t0.692\t0.687\n",
            "PCA-16   \t0.124s\t0.753\t0.762\t0.758\n",
            "PCA-32   \t0.099s\t0.699\t0.718\t0.708\n",
            "PCA-48   \t0.228s\t0.627\t0.667\t0.647\n",
            "PCA-64   \t0.270s\t0.593\t0.659\t0.624\n",
            "__________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "jo9kuYrKvHnT",
        "outputId": "190a3d21-ad64-4336-a91e-596fedaceb76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}